# Sparse-GPT-Pretraining
A codebase for pretraining multi-billion-scale sparse GPTs.
To set up your environment, see `environment_setup.md`.

# Citation
If you find this repository helpful, please cite our work:
```
@software{cui2025sparse,
  title  = {Sparse-GPT-Pretraining: A codebase for pretraining multi-billion-scale sparse language models},
  author = {Cui, Chenwei and Herrera, Benjamin Joseph and Jackson, Rockwell and Kerner, Hannah},
  url    = {https://github.com/kerner-lab/Sparse-GPT-Pretraining},
  month  = nov,
  year   = {2025}
}
```
